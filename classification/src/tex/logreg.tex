\documentclass[specialist,
substylefile = spbu.rtx,
subf,href,colorlinks=true, 12pt]{disser}

\usepackage[a4paper,
mag=1000, includefoot,
left=3cm, right=1.5cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm]{geometry}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{float}

\ifpdf\usepackage{epstopdf}\fi
\let\vec=\mathbf
\widowpenalty=10000
\clubpenalty=10000
% Включать подсекции в оглавление
\setcounter{tocdepth}{2}

\graphicspath{{fig/}}


\newtheorem{theorem}{Утверждение}
\newtheorem{example}{Пример}
\newtheorem{definition}{Определение}
\newtheorem{task}{Задача}
\newtheorem{def1}{Определение}
\newtheorem{proposition}{Утверждение}
\newtheorem{notice}{Замечание}
\newtheorem{task_regul}{Задача с регуляризацией}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\argmax}{arg\,max} 


%-------------------------------------------------------------------------------------
\begin{document}

\include{commands}

%
% Титульный лист на русском языке
%

% Название организации
\institution{%
	Санкт-Петербургский государственный университет \\
	Прикладная математика и информатика \\
	Статистическое моделирование
}

\title{Третьякова Александра Леонидовна \\ Волканова Маргарита Дмитриевна \\
	 Федоров Никита Алексеевич}

% Тема
\topic{\normalfont\scshape %
  Обучение с учителем. Классификация. Функция риска. Логистическая регрессия. Feature selection и extraction}

% Автор
\author{}


% Город и год
\city{Санкт-Петербург}
\date{\number\year}
\maketitle
\tableofcontents

\section{Обучение с учителем. Постановка задачи}
% $\mathsf{X}$ --- множество объектов, $\mathsf{Y}$ --- множество ответов \\
% $\mathsf{y}: \mathsf{X}\to \mathsf{Y}$ --- неизвестная зависимость (target function) \\
% ~\\
% Дано: обучающая выборка --- $\mathsf{(x_1,\ldots, x_n) \subset X}$, $\mathsf{y_i=y(x_i), ~i=1,\ldots,n}$ --- известные ответы. \\
% Найти: $\mathsf{a:X\to Y}$ --- функцию (decision function), приближающую $\mathsf{y}$ на всем множестве $\mathsf{X}$. \\
%   ~\\
% Вероятностная постановка задачи: имеется неизвестное распределение на множестве $\mathsf{X\times Y}$ с плотностью $\mathsf{p(x,y)}$, из которого случайно выбираются $X_n=\mathsf{(x_i,y_i)_{i=1}^n}$ (независимые). \\
%  ~\\
% Задача классификации: 
% \begin{itemize}
% 	\item $\mathsf{Y}=\{-1,+1\}$ --- классификация на 2 класса
% 	\item $\mathsf{Y}=\{1,\ldots,K\}$ --- классификация на $K$ классов
% \end{itemize}
%Обучающая выборка: $X_n=\mathsf{(x_i,y_i)_{i=1}^n},~\mathsf{x_i}\in \mathbb{R}^\mathsf{p},~ \mathsf{y_i}\in \{-1,+1\}.$
%
%%--------------------------------------------------------------------------------------
Пусть наблюдается некоторый количественный отклик $Y$ и $p$ предикторов (признаков) $X_{1},\ldots,X_{p}$. 
Будем предполагать, что между $Y$ и $X=(X_{1},\ldots,X_{p})$ существует определенная связь, которую можно представить в виде
\begin{equation*}
Y
=
f^{\ast}(X)+\varepsilon,
\end{equation*}
где $f^{\ast}$ --- фиксированная, но неизвестная функция от предикторов, $\varepsilon$ --- ошибка, которая не зависит от $X$ и имеет нулевое среднее значение.

Так как ошибки имеют нулевое среднее, можем предсказывать $Y$ в соответствии с формулой 
\begin{equation*}
\hat{Y}
=
\hat{f}(X, \theta),
\end{equation*}
где $\hat{f}$ --- оценка $f^{\ast}$, $\theta$ --- вектор параметров оценки, $\hat{Y}$ --- предсказанное значение $Y$.

Когда целью является предсказание, нам не важна точная форма функции $\hat{f}$, если она обеспечивает точные предсказания $Y$.

Пусть имеется выборка из $n$ отдельных наблюдений: $x_{1},\ldots,x_{n}$ для которых известны соответствующие значения отклика. Обозначим $x_{ij}$ --- значение $j$-го признака $i$-го наблюдения, $x_{i}=(x_{i1},\ldots,x_{ip})\tr$,  $y_{i}$ --- отклик у $i$-го наблюдения.
 
 Хотим найти такую функцию $\hat{f}$, что $y\approx\hat{f}(x)$ для любого наблюдения $(x,y)$.  
 Совокупность $X_{n}$ пар $(x_{1}, y_{1}),\ldots, (x_{n},y_{n})$, которая участвует в оценке функции $f^{\ast}$, называется обучающей выборкой.
 Выборка $X_{k}^{\prime}=(x_{i}^{\prime},y_{i}^{\prime})_{i=1}^{k}$, не участвующая в оценке функции $f^{\ast}$, называется тестовой (или контрольной).

{\bf Вероятностная постановка задачи:} $Y \sim Ber(\sigma(X))$ при классификации на два класса и $Y \sim Mult(\mathbf{\sigma}(X))$, где $\mathbf{\sigma}(X) = (\sigma_1(X), \ldots, \sigma_K(X))$ при классификации на $K$ классов. Задача --- построить оценку $\hat{\sigma} = \hat{\sigma}(X, \theta)$ параметра $\sigma$. Предсказание $\hat{Y} = \text{argmax}_k \ \sigma_k(X)$.
%--------------------------------------------------------------------------------------

\section{Функция риска}

Для оценки качества предсказания необходимо ввести функционал качества. Для этого сначала введём функцию потерь.
\begin{definition}
	Функция потерь (loss function) $L((x, y), \theta)$ --- неотрицательная функция, характеризующая величину ошибки предсказания на объекте $x$. 
\end{definition}

Для задачи классификации в качестве функции потерь обычно используется индикатор ошибки $L((x, y), \theta) = \delta(y, \hat{f}(x, \theta))$.

Теперь определим эмпирический риск --- функционал, который используется для оценки качества предсказания.

\begin{definition}
	Эмпирический риск $Q(X_n, \theta) = \displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^{n} L((x_i, y_i), \theta)$
\end{definition}



%Введём несколько обозначений:
%\begin{itemize}
%\item $\mathsf{f(x,\theta)}$ --- разделяющая (дискриминантная) функция, $\mathsf{\theta}\in\mathbb{R}^\mathsf{p}$.
%
%\item $\mathsf{a(x,\theta)=\sign f(x,\theta)}$ --- классификатор.
%
%\item $\mathsf{f(x,\theta)}=0$ --- разделяющая поверхность.
%
%\item $\mathsf{M_i(\theta)=y_if(x_i,\theta)}$ --- {\bf отступ} объекта $\mathsf{x_i}$. 
%
%\item Если $\mathsf{M_i(\theta)<0},$ то классификатор ошибается на $\mathsf{x_i}$.
%
%\end{itemize}

%\begin{task}
%	$Q(\theta)=\sum \limits_{i=1}^n {[M_i(\theta)<0]} \le \tilde Q(\theta)=\sum \limits_{i=1}^n \mathcal{L}(M_i(\theta)) \to \min \limits_{\theta}$
%\end{task}
%{\bf Метод:} метод стохастического градиента.
%\begin{figure}[h]
%	\begin{center}
%		\begin{minipage}[h]{0.55\linewidth}
%			\includegraphics[width=1\linewidth]{Margins}
%			\caption{}
%			\label{series_IRLS} %% метка рисунка для ссылки на него
%		\end{minipage}
%	\end{center}
%\end{figure}



%	\section{Примеры функции потерь}
%	\begin{itemize}
%	\item Пороговая функция потерь: $\mathsf{[M_i(\theta)<0]}$
%	\item Логарифмическая: $\mathsf{\log_2 (1+e^{-M_i(\theta)})}$
%	\item Экспоненциальная: $\mathsf{e^{-M_i(\theta)}}$
%	\item Кусочно-линейная: $\mathsf{(1-M_i(\theta))_{+}}$
%	\item Квадратичная: $\mathsf{(1-M_i(\theta))^2}$ 
%
%	 \end{itemize}
%\begin{figure}[h]
%	\begin{center}
%		\begin{minipage}[h]{0.9\linewidth}
%			\includegraphics[width=1\linewidth]{lossfunc}
%			\caption{Примеры функции потерь $\mathcal{L}(M)$}
%			\label{series_IRLS} %% метка рисунка для ссылки на него
%		\end{minipage}
%	\end{center}
%\end{figure}





%\section{Линейный классификатор}
%$\mathsf{f_j:X\to\mathbb{R},~j=1,\ldots,p}$
%Линейная модель классификации: 
% \begin{equation*}
% \mathsf{a(x,\theta)=\sign (\sum\limits_{j=1}^p\theta_jf_j(x)-\theta_0),~~~\theta_0,\theta_1,\ldots,\theta_p\in\mathbb{R}}.
% \end{equation*}
% Пусть $\mathsf{f_0=-1}$, тогда
% \begin{equation*}
% \mathsf{a(x,\theta)=\sign\langle \theta,x\rangle ,~ x,\theta\in\mathbb{R}^{p+1}}.
% \end{equation*} 
% $\mathsf{M_i(x)=y_i\langle \theta,x_i\rangle}$ --- отступ объекта $\mathsf{x_i}$. 
% 
% \begin{task}
%$\mathsf{Q(\theta)=\sum \limits_{i=1}^n [y_i\langle \theta,x_i\rangle <0] \le \sum \limits_{i=1}^n \mathcal{L}(y_i\langle \theta,x_i\rangle) \to \min \limits_{\theta}}$
% \end{task}
%Проверка по тестовой выборке $\tilde{\mathbb{X}}_\mathsf{k}=\mathsf{(\tilde{x}_i,\tilde{y}_i)_{i=1}^k}$:
%\begin{equation*}
%\mathsf{\bar{Q}(\theta)=\frac{1}{k}\sum\limits_{i=1}^k [\tilde{y}_i\langle \theta,\tilde{x}_i\rangle <0]}
%\end{equation*}



%\section{Решение задачи оптимизации}
%Для решения задачи оптимизации используется {\bf метод стохастического градиента}.
%
%Вход: $X_n=\mathsf{(x_i,y_i)_{i=1}^n, h, \lambda}$
%Выход: $\mathsf{\theta}$
%\begin{enumerate}
%	\item Инициализация $\mathsf{\theta_j,~ j=0,\ldots,p}$
%	\item Инициализация $\mathsf{\bar{Q}(\theta)}$
%	\item {Повторять
%		\begin{enumerate}
%			\item Выбор $\mathsf{x_i}$ из $X_n$ случайным образом
%			\item Вычисление $\mathsf{\varepsilon_i := \mathcal{L}_i(\theta)}$
%			\item Градиентный шаг $\mathsf{\theta:=\theta-h\nabla\mathcal{L}_i(\theta) }$
%			\item Вычисление $\mathsf{\bar{Q}(\theta):=\lambda\varepsilon_i+(1-\lambda)\bar{Q}(\theta)}$
%		\end{enumerate} 
%	пока $\mathsf{\bar{Q}(\theta)}$ или $\mathsf{\theta}$ не сойдутся
%	}
%\end{enumerate}


\section{Регуляризация}
{\bf Проблемы:}
\begin{itemize}
\item Признаков намного больше, чем объектов
\item Мультиколлинеарность признаков: \\
Пусть $\hat{f}(x,\theta)=\sign (\theta_1f_1(x)+\theta_2f_2(x)-\theta_0)$, $f_2(x)=kf_1(x)$. \\ Тогда $\theta_1f_1(x)+\theta_2f_2(x)=(\theta_1+\beta)f_1(x)+(\theta_2-k\beta)f_2(x) \ \forall \beta$
\end{itemize}
Таким образом, очень много различных векторов дадут близкие значения функционала качества, но при этом коэффициенты могут существенно отличаться. Признаком такого явления может являться большая $\Arrowvert \theta \Arrowvert$. \\ 
{\bf Задача с регуляризацией:}
	\[\widetilde{Q}(X_n, \theta)=Q(X_n, \theta)+\frac{\tau}{2}\Arrowvert \theta \Arrowvert ^2 \to \min \limits_\theta\]


\section{Связь с принципом максимума правдоподобия}
Рассмотрим {\bf модель}: пусть $\mathsf{Y}$ --- вероятностное пространство с плотностью $p(y|x, \theta)$, то есть $\mathcal{L}(Y)$ --- семейство распределений, зависящее от параметров $X$ и $\theta$.
	
Пусть $X_n = (x_i,y_i)_{i=1}^n$, и $y_i$ --- независимы, одинаково распределены. 
	\begin{itemize}
		\item {\bf Максимизация правдоподобия:}
		\[\mathsf{L}(\theta;X_n)=\ln\prod\limits_{i=1}^n p(y_i| x_i, \theta)=\sum\limits_{i=1}^n \ln p(y_i|x_i, \theta) \to \max\limits_\theta.\]
		
		\item {\bf Минимизация аппроксимированного эмпирического риска:} 
			\[\tilde{Q}(X_n, \theta)=\sum\limits_{i=1}^n L(x_i, y_i, \theta) \to \min\limits_\theta.\]
		
	\end{itemize}
Эти задачи эквивалентны, если положить $-\ln p(y_i| x_i, \theta) = L(x_i, y_i, \theta)$. \\
{\bf Пример:} 
\begin{itemize}
\item $p(y| x, \theta)=\frac{1}{1+\exp(- y\langle x,\theta \rangle )}$ --- сигмоидная функция.
\item $L(x, y, \theta)=\log (1+exp(-y\langle x,\theta \rangle))$ ---  логарифмическая функция потерь.
\end{itemize}

{\bf Принцип максимума правдоподобия:}
Пусть $\theta \sim p(\theta; \gamma)$.
\begin{equation*}
\mathsf{L}(X_n, \theta)=\sum\limits_{i=1}^n p(y_i|x_i, \theta)+\underbrace{\ln p(\theta;\gamma)}_{\text{регуляризатор}} \to \max\limits_{\theta,\gamma}
\end{equation*}
{\bf Примеры:} \begin{enumerate}
\item {\bf Гауссовский регуляризатор}: \[\mathsf{p(\theta;\sigma)=\frac{1}{(2\pi\sigma)^{p/2}}\exp{-\frac{\Arrowvert \theta \Arrowvert ^2}{2\sigma}}},\] тогда \[-\ln \mathsf{p(\theta;\sigma)=\frac{1}{2\sigma}\Arrowvert \theta \Arrowvert ^2+\text{const}~~(\tau=1/\sigma)}\]
\item {\bf Регуляризатор Лапласа} (приводит к отбору признаков): \[\mathsf{p(\theta;C)=\frac{1}{(2C)^{p}}\exp{-\frac{\Arrowvert \theta \Arrowvert _1}{C}}},\] тогда \[-\ln \mathsf{p(\theta;C)=\frac{1}{C}\sum\limits_{j=1}^p|\theta_j|+\text{const}~~(\tau=1/C)}\]
\end{enumerate}


\section{Регуляризатор Лапласа. Отбор признаков}
Задача: \[Q(X_n, \theta)=\sum\limits_{i=1}^n \ln p(y_i| x_i, \theta)+\frac{1}{C}\sum\limits_{j=1}^p|\theta_j| \to \min\limits_{\theta,C}.\]
Замена: \[\begin{cases} \mathsf{u_j=\frac{1}{2}(|\theta_j|+\theta_j)} \\ \mathsf{v_j=\frac{1}{2}(|\theta_j|-\theta_j)} \end{cases},\] тогда \[\begin{cases} \mathsf{\theta_j=u_j-v_j} \\ \mathsf{|\theta_j|=u_j+v_j} \end{cases}\]
\begin{equation*}
\begin{cases}
Q(\mathsf{u,v)=\sum \limits_{i=1}^n \mathcal{L}(M_i(u-v,\theta_0))+\frac{1}{C}\sum\limits_{j=1}^p (u_j+v_j) \to \min\limits_{u,v}} \\
\mathsf{u_j\ge 0,~v_j\ge 0,~~ j=1,\ldots,p}
\end{cases}
\end{equation*}
При уменьшении $\mathsf{C}$ (возрастании $\mathsf{\frac{1}{C}}$) обнуляются $\mathsf{u_j}$ и $\mathsf{v_j}$ для все большего количества $\mathsf{j}$, то есть $\mathsf{\theta_j=0}$ и признак не учитывается. 
При $\mathsf{C\to 0}$ выбросим все признаки.


\section{Логистическая регрессия. Подход через минимизацию функции потерь}
Линейная модель классификации: 
\begin{itemize}
\item $\mathsf{\hat{f}(x)=\sign \langle \theta,x\rangle,~~ x,\theta\in \mathbb{R}^p}$
\item $\mathsf{M=\langle \theta,x \rangle y}$ --- отступ.
\end{itemize}
В качестве аппроксимации пороговой функции потерь берется логарифмическая функция потерь $\mathcal{L}(\mathsf{M})=\log(1+e^{-\mathsf{M}}).$
%\begin{task}
%	$\mathsf{Q(\theta)=\sum \limits_{i=1}^n (1+\exp{(-y_i\langle \theta,x_i\rangle)}) \to \min \limits_{\theta}}$
%\end{task}

\begin{figure}[h!]
	\begin{minipage}[h]{0.95\linewidth}
		\center{\includegraphics[width=0.9\linewidth]{loss_function}}
	\end{minipage}
	%\caption{Зависимость сигнала от шума для данных.}
	\label{ris:image1}
\end{figure}

\begin{task}
$\mathsf{Q(X_n, \theta)=\sum \limits_{i=1}^n \log (1+\exp{(-y_i\langle \theta,x_i\rangle)}) \to \min \limits_{\theta}}$
\end{task}
Методы решения задачи минимизации:
\begin{itemize}
	\item метод стохастического градиента
	\item метод Ньютона-Рафсона
\end{itemize}


\section{Логистическая регрессия. Вероятностный подход}
$\mathsf{P(y|x,\theta)=\sigma_\theta(M)=\frac{1}{1+e^{-\langle x,\theta \rangle y}}}$ --- сигмоидная функция. \\
Свойства $\mathsf{\sigma(z)}$:
\begin{itemize}
	\item $\mathsf{\sigma(z)} \in [0,1]$ , задана на $(-\infty,+\infty)$
	\item {$\mathsf{\sigma(z)\to 1,~ z\to +\infty};$ $\mathsf{\sigma(z)\to 0,~ z\to -\infty} $}
	\item $\mathsf{\sigma(z)+\sigma(-z)=1}$
	\item $\mathsf{\sigma ' (z)=\sigma (z)\sigma(-z)}$
	%\item {$\mathsf{\sigma(z)\ge 0.5}$ при $\mathsf{z\ge 0}$ \\
	%$\mathsf{\sigma(z)< 0.5}$ при $\mathsf{z< 0}$}
\end{itemize}
\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.9\linewidth}
			\includegraphics[width=1\linewidth]{sigmoid2}
			\caption{Сигмоидная функция}
			\label{series_IRLS} %% метка рисунка для ссылки на него
		\end{minipage}
		
	\end{center}
\end{figure}
Пусть $\mathsf{Y}=\{0,1\}$.
\begin{itemize}
	\item $\mathsf{P(y_i=1|x;\theta)=\sigma_\theta(x)}$
	\item $\mathsf{P(y_i=0|x;\theta)=1-\sigma_\theta(x)}$
\end{itemize}
Тогда $\mathsf{P(y|x;\theta)=(\sigma_\theta(x))^y(1-\sigma_\theta(x))^{1-y}}.$ \\
Функция правдоподобия:
\begin{equation*}
\mathsf{Q(X_n, \theta)=-\log L(\theta)=-\log \prod\limits_{i=1}^n(\sigma_\theta(x_i))^{y_i}(1-\sigma_\theta(x_i))^{1-y_i}}= 
\end{equation*}
\begin{equation*}
=\mathsf{-\sum\limits_{i=1}^n [y_i\log (\sigma_\theta(x_i)+(1-y_i))\log (1-\sigma_\theta(x_i))]\to \min\limits_{\theta}}
\end{equation*}
%\begin{figure}[h]
%	\begin{center}
%		\begin{minipage}[h]{0.8\linewidth}
%			\includegraphics[width=1.2\linewidth]{cost}
%			%\caption{}
%			\label{series_IRLS} %% метка рисунка для ссылки на него
%		\end{minipage}
%		
%	\end{center}
%\end{figure}


\section{Линейная и логистическая регрессия}

Существуют примеры данных, для которых логистическая регрессия показывает лучшие результаты, чем линейная.
\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.8\linewidth}
			\includegraphics[width=1\linewidth]{regression}
			\caption{Линейная и логистическая регрессия}
			\label{series_IRLS} %% метка рисунка для ссылки на него
		\end{minipage}
	
	\end{center}
\end{figure}

\newpage
\section{Логистическая регрессия. Регуляризация}
$\mathsf{Q(\theta)=-\sum\limits_{i=1}^n [y_i\log (\sigma_\theta(x_i)+(1-y_i))\log (1-\sigma_\theta(x_i))]}$

Регуляризация в логистической регрессии:
\begin{itemize}
	\item {{\bf L2}: 
	$\mathsf{Q_\tau(\theta)=Q(\theta)+\frac{\tau}{2}\sum\limits_{j=1}^p \theta_j^2 \to \min\limits_\theta}$
		}
	\item {{\bf L1}: 
		$\mathsf{Q_\tau(\theta)=Q(\theta)+\tau\sum\limits_{j=1}^p |\theta_j| \to \min\limits_\theta}$
	}
\end{itemize}
Параметр $\mathsf{\tau}$ можно подбирать с помощью кросс-валидации. \\
Методы решения задачи минимизации: 
\begin{itemize}
\item метод стохастического градиента
\item метод Ньютона-Рафсона.
\end{itemize}

	
%\section{Логистическая регрессия. Добавление нелинейных признаков}
%	Можно ли использовать логистическую регрессию в случае, когда нет линейной разделимости?
%	\begin{figure}[h]
%		\begin{center}
%			\begin{minipage}[h]{0.75\linewidth}
%				\includegraphics[width=1\linewidth]{nonlinear}
%				%\caption{Линейная и логистическая регрессия}
%				\label{series_IRLS} %% метка рисунка для ссылки на него
%			\end{minipage}
%			
%		\end{center}
%	\end{figure}
%
%%\begin{minipage}{0.5\textwidth}
%%	$\mathsf{\langle \theta,x\rangle =\theta_1x_1+\theta_2x_2+\theta_0}$ \\
%%	Разделяющая поверхность:
%%	$\mathsf{\theta_1x_1+\theta_2x_2+\theta_0 =0}$
%%\end{minipage}
%%\hfill
%%\begin{minipage}{0.45\textwidth}
%%	
%%	Разделяющая поверхность: \\
%%$\mathsf{\theta_1x_1^2+\theta_2x_2^2+\theta_0 =0}$
%%\end{minipage}
%\begin{equation*}
%\begin{matrix}
%\mathsf{\langle \theta,x\rangle =\theta_1x_1+\theta_2x_2+\theta_0} & \mathsf{\langle \theta,x\rangle =\theta_1x_1^2+\theta_2x_2^2+\theta_0} \\
%\text{Разделяющая поверхность:} & \text{Разделяющая поверхность:} \\
%\mathsf{\theta_1x_1+\theta_2x_2+\theta_0 =0} & 	\mathsf{\theta_1x_1+\theta_2x_2+\theta_0 =0}
%\end{matrix}
%\end{equation*}



\section{Многоклассовая логистическая регрессия}
	Линейный классификатор при произвольном числе классов $\mathsf{Y=\{1,\ldots,K\}}$: 
	\begin{equation*}
	\hat{f}(x,\theta)=\argmax_{y\in Y}\langle \theta_y,x \rangle, \ x,\theta_y\in\mathbb{R}^p
	\end{equation*}
	Вероятность того, что объект $x$ относится к классу $i$:
	\begin{equation*}
	\mathsf{P}(y=i|x;\theta)=\frac{\exp{\langle \theta_y,x \rangle}}{\sum\limits_{z\in Y}\exp{\langle \theta_z,x \rangle}}=\frac{e^{\theta_i^{\mathrm{T}}x}}{\sum\limits_{k=1}^Ke^{\theta_k^{\mathrm{T}}x}}
	\end{equation*}
	Задача:
		\begin{equation*}
	\mathsf{Q(X_n, \theta)=-\sum\limits_{i=1}^n\log P(y_i|x_i;\theta)\to \min\limits_\theta }
	\end{equation*}


\section{Логистическая регрессия. Преимущества и недостатки}
Плюсы:
\begin{enumerate}
	\item Позволяет оценить вероятности принадлежности объектов к классу
	\item Достаточно быстро работает при больших объемах выборки
	\item Применима в случае отсутствия линейной разделимости, если на вход подать полиномиальные признаки
\end{enumerate}
Минусы:
\begin{enumerate}
	\item Плохо работает в задачах, в которых зависимость сложная, нелинейная
\end{enumerate}

%\section{Пример использования логистической регрессии. Задача кредитного скоринга}
%Пусть $\mathsf{Y} = \{+1,-1\}$.
%
%Величина потери $\mathsf{D_{xy}}= \begin{cases} \mathsf{S(x), \ Y=-1 \ (\text{кредит не вернули})} \\ \mathsf{-rS(x)}, \ \mathsf{Y=+1} \ (\text{кредит вернули}) \end{cases}$.
%
%Логистическая регрессия дает возможность вычислять апостериорные вероятности принадлежности классу для каждого объекта $\mathsf{x}$:
%	\begin{equation*}
%	\mathsf{P(y|x;\theta)=\frac{1}{1+e^{-\langle x,\theta \rangle y}}}
%	\end{equation*}
%	
%	$\mathsf{R(x)=\sum\limits_{y\in Y}D_{xy}P(y|x)=\sum\limits_{y\in Y}D_{xy}\sigma_\theta(x)}$ --- оценка мат. ожидания потерь для объекта $\mathsf{x}$.
%Хотим узнать, сколько банк потеряет в худшем случае.
%
%Строим эмпирическую функцию распределения потерь. 
%
%Метод Value at Risk:
%\begin{enumerate}
%	\item {$\mathsf{N}$ раз ($\mathsf{N}=1000$): \begin{itemize}
%	\item	$\forall \mathsf{x_i}$ случайно разыгрываем $\mathsf{y_i\sim P(y|x_i),~~i=1,\ldots,n}$\\
%	\item вычисляем суммарные потери $\mathsf{V}=\sum \limits_{i=1}^n D_{x_iy_i}$
%	\end{itemize}
%	}
%	\item строим эмпирическое распределение величины $\mathsf{V}$
%	\item 99\%-квантиль показывает величину резервируемого капитала
%\end{enumerate}
%\begin{figure}[h]
%	\begin{center}
%		\begin{minipage}[h]{0.9\linewidth}
%			\includegraphics[width=1\linewidth]{Var}
%		%	\caption{Линейная и логистическая регрессия}
%			\label{series_IRLS} %% метка рисунка для ссылки на него
%		\end{minipage}
%		
%	\end{center}
%\end{figure}

\section{Выводы}
\begin{itemize}
	\item Существуют различные варианты аппроксимации пороговой функции потерь, позволяющие использовать методы градиентной оптимизации
	\item Регуляризация решает проблему мультиколлинеарности 
	\item Минимизация аппроксимированного эмпирического риска и максимизация правдоподобия оказываются эквивалентными задачами
	\item Логистическая регрессия позволяет оценить условные вероятности классов
	\item В случае отсутствия линейной разделимости можно добавить нелинейные признаки и использовать логистическую регрессию
\end{itemize}

\end{document}
